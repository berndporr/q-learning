<!DOCTYPE HTML>

<!--Converted with LaTeX2HTML 2019.2 (Released June 5, 2019) -->
<HTML lang="EN">
<HEAD>
<TITLE>Q-Lernen und &ldquo;Tiefes&rdquo; Q-Lernen</TITLE>
<META NAME="description" CONTENT="Q-Lernen">
<META NAME="keywords" CONTENT="Machine Learning, Agents">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">
<META CHARSET="UTF-8">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="viewport" CONTENT="width=device-width, initial-scale=1.0">
<META NAME="Generator" CONTENT="LaTeX2HTML v2019.2">

<LINK REL="STYLESHEET" HREF="q-lernen.css">

</HEAD>

<BODY >

<DIV CLASS="navigation"><!--Navigation Panel-->
<IMG WIDTH="81" HEIGHT="24" ALT="next_inactive" SRC="nx_grp_g.png"> 
<IMG WIDTH="26" HEIGHT="24" ALT="up" SRC="up_g.png"> 
<IMG WIDTH="63" HEIGHT="24" ALT="previous" SRC="prev_g.png">   
<BR>
<BR>
<BR></DIV>
<!--End of Navigation Panel-->

<P>
<H1 class="CENTER">Q-Lernen und &ldquo;Tiefes&rdquo; Q-Lernen</H1>
<P class="CENTER"><STRONG>Bernd Porr</STRONG>
</P>
<HR>

<H1><A ID="SECTION00010000000000000000">
Traditionelles Q-Lernen</A>
</H1>

<P>
Q-lernen ist ein Lernalgorithmus wo eine Agentin selbständig
lernt ihre Belohnung zu maximieren.

<P>

<DIV class="CENTER"><A ID="state_action"></A><A ID="59"></A>
<TABLE>
<CAPTION class="BOTTOM"><STRONG>Abbildung 1:</STRONG>
Zustandsraum und Aktionen einer autonomen Agentin (z.B eine Maus),
  die Futter sucht.
</CAPTION>
<TR><TD>
<DIV class="CENTER">
<IMG STYLE=""
 SRC="img3.svg"
 ALT="\includegraphics[width=\textwidth]{state_action}">

</DIV></TD></TR>
</TABLE>
</DIV>

<P>
Abb.&nbsp;<A HREF="#state_action">1</A> zeigt eine klassische 2D-Welt, in der sich
eine solche Agentin (hier eine Maus) bewegt. Abb.&nbsp;<A HREF="#state_action">1</A>A
zeigt den Zustandsraum. In diesem Beispiel ist der Zustandsraum 2D, in
dem sich die Agentin bewegen kann. Manche Zustände sind durch Wände
verboten und bei anderen gibt es eine Belohnung (als &ldquo;Käse&rdquo;
gekennzeichnet). Zustände werden mit <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img4.svg"
 ALT="$s$"></SPAN> gekennzeichnet und wenn sie
zum Zeitpunkt <SPAN CLASS="MATH"><IMG STYLE="height: 1.57ex; vertical-align: -0.09ex; " SRC="img5.svg"
 ALT="$t$"></SPAN> passieren, dann wird das als <SPAN CLASS="MATH"><IMG STYLE="height: 1.49ex; vertical-align: -0.43ex; " SRC="img6.svg"
 ALT="$s_t$"></SPAN>
gekennzeichnet. Jeder Zustand besitzt auch eine skalare
Belohnungsvariable <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img7.svg"
 ALT="$r$"></SPAN>, die positiv <SPAN CLASS="MATH"><IMG STYLE="height: 1.69ex; vertical-align: -0.15ex; " SRC="img8.svg"
 ALT="$r&gt;0$"></SPAN> ist, wenn es eine Belohnung gibt
und negativ <SPAN CLASS="MATH"><IMG STYLE="height: 1.69ex; vertical-align: -0.15ex; " SRC="img9.svg"
 ALT="$r&lt;0$"></SPAN>, wenn der Zustand bestraft werden soll. Für alle anderen
Zustände ist <SPAN CLASS="MATH"><IMG STYLE="height: 1.63ex; vertical-align: -0.09ex; " SRC="img10.svg"
 ALT="$r=0$"></SPAN>.

<P>
Abb.&nbsp;<A HREF="#state_action">1</A>B zeigt die möglichen Aktionen <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img11.svg"
 ALT="$a$"></SPAN>, die die
Agentin durchführen kann. Eine Aktion bewirkt eine Bewegung von einem
Zustand <SPAN CLASS="MATH"><IMG STYLE="height: 1.49ex; vertical-align: -0.43ex; " SRC="img6.svg"
 ALT="$s_t$"></SPAN> zum Zeitpunkt <SPAN CLASS="MATH"><IMG STYLE="height: 1.57ex; vertical-align: -0.09ex; " SRC="img5.svg"
 ALT="$t$"></SPAN> zum Zustand <SPAN CLASS="MATH"><IMG STYLE="height: 1.64ex; vertical-align: -0.59ex; " SRC="img12.svg"
 ALT="$s_{t+1}$"></SPAN> und evtl gibt
es eine Belohnung (<SPAN CLASS="MATH"><IMG STYLE="height: 1.69ex; vertical-align: -0.15ex; " SRC="img8.svg"
 ALT="$r&gt;0$"></SPAN>) oder eine Bestrafung (<SPAN CLASS="MATH"><IMG STYLE="height: 1.69ex; vertical-align: -0.15ex; " SRC="img9.svg"
 ALT="$r&lt;0$"></SPAN>).

<P>
Abb.&nbsp;<A HREF="#state_action">1</A>C illustriert, wie man die Interaktion zwischen
der autonomen Agentin und Umwelt als geschlossenes System interpretieren. Jede
Aktion <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img11.svg"
 ALT="$a$"></SPAN> der Agentin erzeugt einen neuen Zustand <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img4.svg"
 ALT="$s$"></SPAN> und
gegebenenfalls auch eine Belohnung <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img7.svg"
 ALT="$r$"></SPAN>.

<P>

<DIV class="CENTER"><A ID="learning_steps"></A><A ID="61"></A>
<TABLE>
<CAPTION class="BOTTOM"><STRONG>Abbildung 2:</STRONG>
Lernschritte von Q-learning. A) Die Agentin befindet
  sich direkt auf der Belohnung. Das führt dazu, dass alle
  Q-Werte gleichermaßen erhöht werden, z.B. auf <SPAN CLASS="MATH"><IMG STYLE="height: 1.63ex; vertical-align: -0.09ex; " SRC="img1.svg"
 ALT="$0.5$"></SPAN> mit
  einer Lernrate von <!-- MATH
 $\alpha = 0.5$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 1.63ex; vertical-align: -0.09ex; " SRC="img2.svg"
 ALT="$\alpha = 0.5$"></SPAN>.
</CAPTION>
<TR><TD>
<DIV class="CENTER">
<IMG STYLE=""
 SRC="img13.svg"
 ALT="\includegraphics[width=0.75\textwidth]{learning_steps}">

</DIV></TD></TR>
</TABLE>
</DIV>

<P>
Die Frage stellt sich nun, wie die Agentin von jeder beliebigen Stelle
<SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img4.svg"
 ALT="$s$"></SPAN> die Belohnungen findet. Das wird erreicht, indem jede Aktion <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img11.svg"
 ALT="$a$"></SPAN>
bezüglich eines Zustandes <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img4.svg"
 ALT="$s$"></SPAN> eine Hilfsvariable erhält, die wir
<SPAN CLASS="MATH"><IMG STYLE="height: 2.43ex; vertical-align: -0.66ex; " SRC="img14.svg"
 ALT="$Q(s,a)$"></SPAN> nennen (siehe Abb.&nbsp;<A HREF="#learning_steps">2</A>A). Jede Aktion <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img11.svg"
 ALT="$a$"></SPAN> in einem
Zustand <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img4.svg"
 ALT="$s$"></SPAN> erhält damit verschiedene Q-Werte. Das Ziel ist es, den Aktionen
einen hohen Q-Wert zu geben, die maximale Belohnung versprechen. In diesem
Beispiel erhält jeder Zustand <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img4.svg"
 ALT="$s$"></SPAN> vier Q-Werte (Nord, West, Ost
und Süd). Anfänglich sind alle Q-Werte null werden dann mit Hilfe der iterativen Bellmangleichung bestimmt:
<P></P>
<DIV CLASS="mathdisplay"><A ID="bellit"></A><!-- MATH
 \begin{equation}
Q(s,a) \leftarrow Q(s,a) + \alpha \underbrace{\left[ r(s) + \gamma \max_{a^\prime} Q(s^\prime,a^\prime) - Q(s,a) \right]}_{\delta(s,a)}
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 7.89ex; vertical-align: -5.24ex; " SRC="img15.svg"
 ALT="$\displaystyle Q(s,a) \leftarrow Q(s,a) + \alpha \underbrace{\left[ r(s) + \gamma \max_{a^\prime} Q(s^\prime,a^\prime) - Q(s,a) \right]}_{\delta(s,a)}$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">1</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
wo <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img16.svg"
 ALT="$\alpha$"></SPAN> die Lernrate ist und <!-- MATH
 $0 < \gamma < 1$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.07ex; vertical-align: -0.53ex; " SRC="img17.svg"
 ALT="$0 &lt; \gamma &lt; 1$"></SPAN> der &ldquo;discount
Factor&rdquo; der zukünftige Belohnungen abwertet. Für die Beispiele hier
nehmen wir einfach an, dass <!-- MATH
 $\alpha = 0.5$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 1.63ex; vertical-align: -0.09ex; " SRC="img2.svg"
 ALT="$\alpha = 0.5$"></SPAN> und <!-- MATH
 $\gamma = 1$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.07ex; vertical-align: -0.53ex; " SRC="img18.svg"
 ALT="$\gamma = 1$"></SPAN> ist.

<P>
Die Q-Werte werden nun iterativ gelernt, wobei die Agentin
Zufallsaktionen <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img11.svg"
 ALT="$a$"></SPAN> durchführt und dann <SPAN CLASS="MATH"><IMG STYLE="height: 2.43ex; vertical-align: -0.66ex; " SRC="img14.svg"
 ALT="$Q(s,a)$"></SPAN> aktualisiert
wird. Die Agentin kann immer einen Schritt voraussehen, also quasi
über die Schwelle zu einem anderen Zustand gucken. Das wird in Q-Lernen
&ldquo;Beobachtung genannt&rdquo;. Mehr kann die Agentin nicht sehen. Am Anfang
sind all Q-Werte null. Nur die direkte Belohnung kann diesen Q-Wert von Null
erhöhen, was in Abb.&nbsp;<A HREF="#learning_steps">2</A>A gezeigt wird. Bei einer
Lernrate von <!-- MATH
 $\alpha = 0.5$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 1.63ex; vertical-align: -0.09ex; " SRC="img2.svg"
 ALT="$\alpha = 0.5$"></SPAN> ergibt das dann für alle Aktionen einen
Q-Wert von <SPAN CLASS="MATH"><IMG STYLE="height: 1.63ex; vertical-align: -0.09ex; " SRC="img1.svg"
 ALT="$0.5$"></SPAN>.

<P>
Der entscheidende Trick ist aber nun, wenn bei der nächsten
Zufallswanderung die simulierte Maus einen Schritt vor der primären
Belohnung steht und <SPAN CLASS="MATH"><IMG STYLE="height: 2.43ex; vertical-align: -0.66ex; " SRC="img14.svg"
 ALT="$Q(s,a)$"></SPAN> nicht mehr überall null
ist. Abb.&nbsp;<A HREF="#learning_steps">2</A>B zeigt nun die Agentin einen Schritt
vor der Belohnung. Die Maus schaut also nun in die verschiedenen
Felder um sich herum und bestimmt dort den maximalen Q-Wert
<!-- MATH
 $\max_{a^\prime} Q(s^\prime,a^\prime)$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.43ex; vertical-align: -0.66ex; " SRC="img19.svg"
 ALT="$\max_{a^\prime} Q(s^\prime,a^\prime)$"></SPAN>, vergleicht den mit dem
aktuellen Q-Wert <SPAN CLASS="MATH"><IMG STYLE="height: 2.43ex; vertical-align: -0.66ex; " SRC="img14.svg"
 ALT="$Q(s,a)$"></SPAN> und korrigiert dann den aktuellen Q-Wert
anhand der Lernrate. Aus diesem Grunde wird der Term <!-- MATH
 $\delta(s,a)$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.43ex; vertical-align: -0.66ex; " SRC="img20.svg"
 ALT="$\delta(s,a)$"></SPAN>
auch Vorhersagefehler oder im Englischen &ldquo;Reward Prediction Error&rdquo;
(RPE) genannt.

<P>
Das Experiment in Abb.&nbsp;<A HREF="#learning_steps">2</A> wird nun mit der
Formel.&nbsp;<A HREF="#bellit">1</A> viele Male mit Zufallswanderungen bei einer
maximalen Laufzeit <SPAN CLASS="MATH"><IMG STYLE="height: 1.72ex; vertical-align: -0.09ex; " SRC="img21.svg"
 ALT="$T$"></SPAN> wiederholt, bis der Fehler <SPAN CLASS="MATH"><IMG STYLE="height: 1.74ex; vertical-align: -0.09ex; " SRC="img22.svg"
 ALT="$\delta$"></SPAN> im Mittel
Null ist also die Q-Werte sich nicht mehr ändern. Das Endergebnis gibt
dann eine Vorhersage der gesamten Belohnung in Abhängigkeit von <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img4.svg"
 ALT="$s$"></SPAN>
und <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img11.svg"
 ALT="$a$"></SPAN>:
<P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
Q(s,a) = \sum_{t=0}^T \gamma^t r_t
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 7.04ex; vertical-align: -2.91ex; " SRC="img23.svg"
 ALT="$\displaystyle Q(s,a) = \sum_{t=0}^T \gamma^t r_t$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">2</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
wo <SPAN CLASS="MATH"><IMG STYLE="height: 1.72ex; vertical-align: -0.09ex; " SRC="img21.svg"
 ALT="$T$"></SPAN> die Gesamtzeit ist, die Aufgabe zu erledigen.

<P>
Soweit haben wir nur eine Matrix von Q-Werten aber wie kann diese
Matrix nun verwendet werden, um schnell alle Belohnungen zu sammeln
und Bestrafungen zu vermeiden? Einfach indem die Agentin sie immer in
den Zustand <SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img4.svg"
 ALT="$s$"></SPAN> springt, welches den höchsten Q-Wert hat. Das nennt
sich eine &ldquo;Policy&rdquo; oder &ldquo;Strategie&rdquo;. Solch eine Strategie
nennt sich &ldquo;gierig&rdquo; oder &ldquo;greedy&rdquo;, da sie immer die lokal stärkste
Belohnung erarbeitet.

<P>
Praktisch gesehen, werden normalerweise das Lernen von Q
(&ldquo;exploration&rdquo;) und das Ausführen der Strategie (&ldquo;exploitation&rdquo;)
gemischt.

<P>

<H1><A ID="SECTION00020000000000000000">
Tiefes Q-Lernen</A>
</H1>
Beim tiefen Q-lernen wird <SPAN CLASS="MATH"><IMG STYLE="height: 2.43ex; vertical-align: -0.66ex; " SRC="img14.svg"
 ALT="$Q(s,a)$"></SPAN> nicht iterativ anhand einer Q-Tabelle
ausgerechnet sondern mit Hilfe eines Deep Neuronal Nets. Dieses Netz
berechnet dann gleichzeitig alle Q-Werte bezüglich eines
Zustandes. Z.B in Abb.&nbsp;<A HREF="#learning_steps">2</A> sitzt die Maus im Zustand
<SPAN CLASS="MATH"><IMG STYLE="height: 1.15ex; vertical-align: -0.09ex; " SRC="img4.svg"
 ALT="$s$"></SPAN> und kann 4 Aktionen ausführen. Ein Neuronales Netz kann dann die
Q-Werte für <!-- MATH
 $Q(s,\textrm{Nord}), Q(s,\textrm{Sued}),
Q(s,\textrm{West}), Q(s,\textrm{Ost})$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.43ex; vertical-align: -0.66ex; " SRC="img24.svg"
 ALT="$Q(s,\textrm{Nord}), Q(s,\textrm{Sued}),
Q(s,\textrm{West}), Q(s,\textrm{Ost})$"></SPAN> gleichzeitig ausgeben und dann
kann die Agentin anhand der &ldquo;gierigen&rdquo; Strategie die Aktion mit dem
höchsten Q-Wert ausführen lassen.

<P>
Wie wird bei deep Q-learning gelernt? Wenn man sich
Gl.&nbsp;<A HREF="#bellit">1</A> ansieht ist Lernen nichts anderes als den
Fehler <!-- MATH
 $\delta(s,a)$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.43ex; vertical-align: -0.66ex; " SRC="img20.svg"
 ALT="$\delta(s,a)$"></SPAN> mit Hilfe von Error-Backpropagation durch das
Netz zu schicken, was jedes Deep Net erledigen kann,
z.B. Tensorflow. Weil Deep Nets mit einem grossen Zustandsraum
klarkommen kann man diese nicht nur mit x/y-Koordinaten am Eingang
füttern sondern z.B. einfach die Vogelperspektive des ganzen Gitters
von Abb.&nbsp;<A HREF="#state_action">1</A> zur Verfügung stellen. Das wurde z.B. bei
Deep Minds's Atari-Game gemacht.

<P>

<H1><A ID="SECTION00030000000000000000">
&Uuml;ber dieses Dokument ...</A>
</H1>
 <STRONG>Q-Lernen und &ldquo;Tiefes&rdquo; Q-Lernen</STRONG><P>
This document was generated using the
<A HREF="http://www.latex2html.org/">LaTeX2HTML</A> translator Version 2019.2 (Released June 5, 2019)
<P>
The command line arguments were: <BR>
 <kbd>latex2html -init_file latex2html.config q-lernen</kbd>
<P>
The translation was initiated on 2022-04-28
<BR> <HR>
<!--Table of Child-Links-->
<A ID="CHILD_LINKS"></A>

<UL CLASS="ChildLinks">
<LI><A ID="tex2html3"
  HREF="q-lernen.html#SECTION00010000000000000000">Traditionelles Q-Lernen</A>
<LI><A ID="tex2html4"
  HREF="q-lernen.html#SECTION00020000000000000000">Tiefes Q-Lernen</A>
<LI><A ID="tex2html5"
  HREF="q-lernen.html#SECTION00030000000000000000">&Uuml;ber dieses Dokument ...</A>
</UL>
<!--End of Table of Child-Links-->

<DIV CLASS="navigation"><HR></DIV>
<!--End of Navigation Panel-->
<p><a href="https://github.com/berndporr/rt_embedded5_teaching/">github / contact</a></p><P><A REL="license" HREF="http://creativecommons.org/licenses/by-sa/4.0/"><IMG ALT="Creative Commons License" SRC="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></A></P>
</BODY>
</HTML>
