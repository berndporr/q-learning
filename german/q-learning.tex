\documentclass[12pt]{report}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{%
    pdfborder = {0 0 0}
}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=blue,
    linkcolor=blue,
    urlcolor=blue
}
\renewcommand{\familydefault}{\sfdefault}
\renewcommand{\captionfont}{\small}

\author{Bernd Porr}
\title{Q-learning und Deep Q-learning}

\begin{document}

\maketitle

Q-learning ist ein Lernalgorithmus wo eine Agentin selbständig
lernt ihre Belohnung zu maximieren.

\begin{figure}[!hbt]
\begin{center}
\mbox{\includegraphics[width=\textwidth]{state_action}}
\end{center}
\caption{Zustandsraum und Aktionen eines autonomen Agenten,
  der Futter sucht.
\label{state_action}}
\end{figure}

Abb.~\ref{state_action} zeigt eine klassische 2D-Welt, in der sich
eine solche Agentin (hier eine Maus) bewegt. Abb.~\ref{state_action}A
zeigt den Zustandsraum. In diesem Beispiel ist der Zustandsraum 2D, in
dem sich die Agentin bewegen kann. Manche Zustände sind durch Wände
verboten und bei anderen gibt es eine Belohnung (also ``Käse''
gekennzeichnet).  Zustände werden mit $s$ gekennzeichet und wenn sie
zum Zeitpunkt $t$ passieren, dann wird das als $s_t$
gekennzeichet. Jeder Zustand besitzt auch eine skalare
Belohungsvariable $r$, die positiv ist, wenn es eine Belohnung gibt
und negativ, wenn der Zustand bestraft werden soll. Fuer alle anderen
Zustaende ist $r=0$.

Abb.~\ref{state_action}B zeigt die möglichen Aktionen $a$, die die
Agentin durchführen kann. Eine Aktion bewirkt eine Bewegung von einem
Zustand $s_t$ zum Zeitpunkt $t$ zum Zustand $s_{t+1}$ und evtl gibt
es eine Belohnung ($r>0$) oder eine Bestrafung ($r<0$).

Abb.~\ref{state_action}C illustriert, wie man die Interaktion
zwischen Agentin und Umwelt als geschlossenes System interpretieren
kann damit die Agentin autonom ist. Jede Aktion $a$ der Agentin
erzeugt einen neuen Zustand $s$ und gegebenenfalls auch eine
Belohnung $r$.

\begin{figure}[!hbt]
\begin{center}
\mbox{\includegraphics[width=0.75\textwidth]{learning_steps}}
\end{center}
\caption{Lernschritte von Q-learning. A) Die Agentin befindet
  sich direkt auf der Belohnung. Das führt dazu, dass alle
  Q-Werte gleichermassen erhöht werden, z.B. auf $0.5$ mit
  einer Lernrate von $\alpha = 0.5$.
\label{learning_steps}}
\end{figure}

Die Frage stellt sich nun, wie die Agentin von jeder beliebigen Stelle
$s$ die Belohnungen findet. Das wird erreicht, indem jede Aktion $a$
bezueglich eines Zustandes $s$ eine Hilfsvariable erhaelt, die wir
$Q(s,a)$ nennen (siehe Abb.~\ref{learning_steps}A). In diesem
Beispiel erhaelt jeder Zustand $s$ vier Q-Werte (Nord, West, Ost
und S\"ud). Anfaenglich sind alle Q-Werte null.

Die Q-Werte werden nun mit Hilfe der iterativen Bellman-Formel bestimmt:
\begin{equation}
  Q(s,a) \leftarrow Q(s,a) + \alpha \underbrace{\left[ r(s) + \gamma \max_{a^\prime} Q(s^\prime,a^\prime) - Q(s,a) \right]}_{\delta(s,a)}
  \label{bellit}
\end{equation}
wo $\alpha$ die Lernrate ist und $0 < \gamma < 1$ der ``discount
Factor'' der zukünftige Belohnungen abwertet. Fuer die Beispiele hier
nehmen wir einfach an, dass $\alpha = 0.5$ und $\gamma = 1$ ist.

Die Q-Werte werden nun iterativ gelernt, wobei die Agentin
Zufallsaktionen $a$ durchführt und dann $Q(s,a)$ aktualisiert wird.
Am Anfang sind all Q-Werte null. Nur die direkte Belohnung kann diesen
von Null erhoehen, was in Abb.~\ref{learning_steps}A gezeigt wird. Bei
einer Lernrate von $\alpha = 0.5$ ergibt das dann fuer alle Aktionen
einen Q-Wert von $0.5$.

Der entscheidende Trick ist aber nun, wenn bei der naechsten
Zufallswanderung die simulierte Maus einen Schritt vor der primaeren
Belohnung steht. Nun ist $Q(s,a)$ nicht mehr überall
null. Abb.~\ref{learning_steps}B zeigt nun die Agentin einen Schritt
vor der Belohnung. $Q(s,a)$ ist der Q-Wert \textsl{nach} der Aktion
$a$ also einen Schritt voraus. Das ist alles, was die Agentin ``sehen
kann'': einen Schritt voraus und das wird im Sinne von Q-learning
``Beobachtung'' oder ``Beobachtungshorizont'' genannt. Die Maus schaut
also nun in die verschiedenen Felder um sich herum und nimmt den
maximalen Q-Wert, vergleicht den mit dem aktuellen Q-Wert und
korrigiert dann den aktuellen Q-Wert anhand der Lernrate. Aus diesem
Grunde wird der Term $\delta(s,a)$ auch Vorhersagefehler oder im
Englischen ``Reward Prediction Error'' (RPE) genannt.

Das Experiment in Abb.~\ref{learning_steps} wird nun mit der
Eq.~\ref{bellit} viele Male mit Zufallswanderungen mit der maximalen
Zeit $T$ wiederholt, bis der Fehler $\delta$ im Mittel Null
ist. Das Endergebnis gibt dann eine Vorhersage der
Belohnung fuer jedes Q in Abhaengigkeit von $s$ und $a$:
\begin{equation}
  Q(s,a) = \sum_{t=0}^T \gamma^t r_t
\end{equation}
wo $T$ die Gesamtzeit ist die Aufgabe zu erledigen.

Soweit haben wir nur einen Matrix von Q-Werten aber wie kann diese
Matrix nun verwendet werden, um schnell alle Belohnungen zu sammeln
und Bestrafungen zu vermeiden? Einfach indem die Agentin sie immer in
den Zustand $s$ springt, welches den hoechsten Q-Wert hat. Das nennt
sich dann eine ``Policy'' oder ``Strategie''. Solch eine Strategie
nennt sich ``gierig'' oder ``greedy'', da sie immer die lokal stärkste
Belohnung erarbeitet.

Praktisch gesehen, werden normalerweise das Lernen von Q
(``exploration'') und das Ausfuehren der Strategie (``explotation'')
gemischt.

In Deep Q-learning wird $Q(s,a)$ nicht iterativ anhand einer Q-Tabelle
ausgerechet sondern mit Hilfe eines Deep Neuronal Nets. Dieses Netz
berechnet dann gleichzeitig alle Q-Werte bezueglich eines
Zustandes. Z.B in Abb.~\ref{learning_steps} sitzt die Maus im Zustand
$s$ und kann 4 Aktionen ausfuehren. Ein Neuronales Netz kann dann die
Q-Werte fuer $Q(s,\textrm{Nord}), Q(s,\textrm{Sued}),
Q(s,\textrm{West}), Q(s,\textrm{Ost})$ gleichzeitig ausgeben und dann
kann die Agentin anhand der ``gierigen'' Strategie die Aktion mit dem
hoechsten Q-Wert ausfuehren lassen.

Wie wird bei deep Q-learning gelernt? Wenn man sich
Gleichung.~\ref{bellit} ansieht ist Lernen nichts anderes als den
Fehler $\delta(s,a)$ mit Hilfe von Error-Backpropagation durch das
Netz zu schicken, was jedes Deep Net erledigen kann,
z.B. Tensorflow. Weil Deep Nets mit einem grossen Zustandsraum
klarkommen kann man diese nicht nur mit x/y-Koordinaten am Eingang
fuettern sondern z.B. einfach die Vogelperspektive des ganzen Gitters
von Abb.~\ref{state_action} zur Verfügung stellen. Das wurde z.B. bei
Deep Minds's Atari-game gemacht.

\end{document}
